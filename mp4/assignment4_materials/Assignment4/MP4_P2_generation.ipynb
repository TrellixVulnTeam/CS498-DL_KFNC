{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "colab": {
      "name": "MP4_P2_generation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWe3hlHp2SwZ"
      },
      "source": [
        "# Generating Text with an RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oV6rEVSwRQGV",
        "outputId": "243faa5a-eea2-46e5-9ad1-67801ecf0feb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import os\n",
        "os.chdir(\"gdrive/My Drive/Colab Notebooks/Assignment4\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gb6hCob-RUXz"
      },
      "source": [
        "!pip install unidecode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXVFbMTw2Swa"
      },
      "source": [
        "import unidecode\n",
        "import string\n",
        "import random\n",
        "import re\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWeMGVNW2Swe"
      },
      "source": [
        "from rnn.model import RNN\n",
        "from rnn.helpers import time_since\n",
        "from rnn.generate import generate"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "W29_qyni2Swi"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyJbX3732Swl"
      },
      "source": [
        "## Data Processing\n",
        "\n",
        "The file we are using is a plain text file. We turn any potential unicode characters into plain ASCII by using the `unidecode` package (which you can install via `pip` or `conda`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eg6CaJo2Swl",
        "outputId": "edaab9e4-5830-4fcf-b379-574b03a1791f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "all_characters = string.printable\n",
        "n_characters = len(all_characters)\n",
        "\n",
        "file_path = './shakespeare.txt'\n",
        "file = unidecode.unidecode(open(file_path).read())\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)\n",
        "\n",
        "# we will leave the last 1/10th of text as test\n",
        "split = int(0.9*file_len)\n",
        "train_text = file[:split]\n",
        "test_text = file[split:]\n",
        "\n",
        "print('train len: ', len(train_text))\n",
        "print('test len: ', len(test_text))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file_len = 4573338\n",
            "train len:  4116004\n",
            "test len:  457334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gvl39-P2Swn",
        "outputId": "b54fc93d-983d-41e3-d694-abe76ad6ca25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "chunk_len = 200\n",
        "\n",
        "def random_chunk(text):\n",
        "    start_index = random.randint(0, len(text) - chunk_len)\n",
        "    end_index = start_index + chunk_len + 1\n",
        "    return text[start_index:end_index]\n",
        "\n",
        "print(random_chunk(train_text))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "e not hurt.\n",
            "\n",
            "Second Lord:\n",
            "\n",
            "CLOTEN:\n",
            "The villain would not stand me.\n",
            "\n",
            "Second Lord:\n",
            "\n",
            "First Lord:\n",
            "Stand you! You have land enough of your own: but\n",
            "he added to your having; gave you some ground.\n",
            "\n",
            "Second Lor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1Ty52j-2Swq"
      },
      "source": [
        "### Input and Target data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMvvjoKJ2Swq"
      },
      "source": [
        "To make training samples out of the large string of text data, we will be splitting the text into chunks.\n",
        "\n",
        "Each chunk will be turned into a tensor, specifically a `LongTensor` (used for integer values), by looping through the characters of the string and looking up the index of each character in `all_characters`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihGX4RyX2Swr"
      },
      "source": [
        "# Turn string into list of longs\n",
        "def char_tensor(string):\n",
        "    tensor = torch.zeros(len(string), requires_grad=True).long()\n",
        "    for c in range(len(string)):\n",
        "        tensor[c] = all_characters.index(string[c])\n",
        "    return tensor"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MM3PSYV2Swu"
      },
      "source": [
        "The following function loads a batch of input and target tensors for training. Each sample comes from a random chunk of text. A sample input will consist of all characters *except the last*, while the target wil contain all characters *following the first*. For example: if random_chunk='abc', then input='ab' and target='bc'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKMUcMVI2Swv"
      },
      "source": [
        "def load_random_batch(text, chunk_len, batch_size):\n",
        "    input_data = torch.zeros(batch_size, chunk_len).long().to(device)\n",
        "    target = torch.zeros(batch_size, chunk_len).long().to(device)\n",
        "    for i in range(batch_size):\n",
        "        start_index = random.randint(0, len(text) - chunk_len - 1)\n",
        "        end_index = start_index + chunk_len + 1\n",
        "        chunk = text[start_index:end_index]\n",
        "        input_data[i] = char_tensor(chunk[:-1])\n",
        "        target[i] = char_tensor(chunk[1:])\n",
        "    return input_data, target"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUvZcfmr2Sw0"
      },
      "source": [
        "# Implement model\n",
        "\n",
        "Your RNN model will take as input the character for step $t_{-1}$ and output a prediction for the next character $t$. The model should consiste of three layers - a linear layer that encodes the input character into an embedded state, an RNN layer (which may itself have multiple layers) that operates on that embedded state and a hidden state, and a decoder layer that outputs the predicted character scores distribution.\n",
        "\n",
        "\n",
        "You must implement your model in the `rnn/model.py` file. You should use a `nn.Embedding` object for the encoding layer, a RNN model like `nn.RNN` or `nn.LSTM`, and a `nn.Linear` layer for the final a predicted character score decoding layer.\n",
        "\n",
        "\n",
        "**TODO:** Implement the model in RNN `rnn/model.py`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moIhaJcN2Sw1"
      },
      "source": [
        "# Evaluating\n",
        "\n",
        "To evaluate the network we will feed one character at a time, use the outputs of the network as a probability distribution for the next character, and repeat. To start generation we pass a priming string to start building up the hidden state, from which we then generate one character at a time.\n",
        "\n",
        "\n",
        "Note that in the `evaluate` function, every time a prediction is made the outputs are divided by the \"temperature\" argument. Higher temperature values make actions more equally likely giving more \"random\" outputs. Lower temperature values (less than 1) high likelihood options contribute more. A temperature near 0 outputs only the most likely outputs.\n",
        "\n",
        "You may check different temperature values yourself, but we have provided a default which should work well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13BMnRpP2Sw1"
      },
      "source": [
        "def evaluate(rnn, prime_str='A', predict_len=100, temperature=0.8):\n",
        "    hidden = rnn.init_hidden(1, device=device)\n",
        "    prime_input = char_tensor(prime_str).unsqueeze(0).to(device)\n",
        "    predicted = prime_str\n",
        "\n",
        "    # Use priming string to \"build up\" hidden state\n",
        "    for p in range(len(prime_str) - 1):\n",
        "        _, hidden = rnn(prime_input[:,p].unsqueeze(1), hidden)\n",
        "    inp = prime_input[:,-1].unsqueeze(1)\n",
        "    \n",
        "    for p in range(predict_len):\n",
        "        output, hidden = rnn(inp, hidden)\n",
        "        \n",
        "        # Sample from the network as a multinomial distribution\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        top_i = torch.multinomial(output_dist, 1)[0]\n",
        "        \n",
        "        # Add predicted character to string and use as next input\n",
        "        predicted_char = all_characters[top_i]\n",
        "        predicted += predicted_char\n",
        "        inp = char_tensor(predicted_char).unsqueeze(0).to(device)\n",
        "\n",
        "    return predicted"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KV6pWvBE2Sw3"
      },
      "source": [
        "# Train RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qf-MUvU92Sw4"
      },
      "source": [
        "batch_size = 128\n",
        "n_epochs = 5000\n",
        "hidden_size = 256\n",
        "n_layers = 1\n",
        "learning_rate = 0.01\n",
        "model_type = 'lstm'\n",
        "print_every = 50\n",
        "plot_every = 50\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bPbe02l2Sw6"
      },
      "source": [
        "def eval_test(rnn, inp, target):\n",
        "    with torch.no_grad():\n",
        "        hidden = rnn.init_hidden(batch_size, device=device)\n",
        "        loss = 0.0\n",
        "        for c in range(chunk_len):\n",
        "            output, hidden = rnn(inp[:,c].unsqueeze(1), hidden)\n",
        "            loss += criterion(output.view(batch_size, -1), target[:,c])\n",
        "    return loss.data.item() / chunk_len"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjEa3Eeq2Sw8"
      },
      "source": [
        "### Train function\n",
        "\n",
        "**TODO**: Fill in the train function. You should initialize a hidden layer representation using your RNN's `init_hidden` function, set the model gradients to zero, and loop over each time step (character) in the input tensor. For each time step compute the output of the of the RNN and compute the loss over the output and the corresponding ground truth time step in `target`. The loss should be averaged over all time steps. Lastly, call backward on the averaged loss and take an optimizer step.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lie7tzR62Sw8"
      },
      "source": [
        "def train(rnn, input, target, optimizer, criterion):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "    - rnn: model\n",
        "    - input: input character data tensor of shape (batch_size, chunk_len)\n",
        "    - target: target character data tensor of shape (batch_size, chunk_len)\n",
        "    - optimizer: rnn model optimizer\n",
        "    - criterion: loss function\n",
        "    \n",
        "    Returns:\n",
        "    - loss: computed loss value as python float\n",
        "    \"\"\"\n",
        "    out_loss = 0.0\n",
        "    \n",
        "    ####################################\n",
        "    #          YOUR CODE HERE          #\n",
        "    ####################################\n",
        "    hidden = rnn.init_hidden(batch_size, device=device)\n",
        "    rnn.train()\n",
        "    optimizer.zero_grad()\n",
        "    loss = 0.0\n",
        "    for c in range(chunk_len):\n",
        "      output, hidden = rnn(input[:,c].unsqueeze(1), hidden)\n",
        "      loss += criterion(output.view(batch_size, -1), target[:,c])\n",
        "    loss /= chunk_len \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    out_loss += loss.item()\n",
        "    ##########       END      ##########\n",
        "\n",
        "    return out_loss\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIy07deD2SxA",
        "outputId": "5cdd195e-de25-4657-ad16-d142a4994707",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "rnn = RNN(n_characters, hidden_size, n_characters, model_type=model_type, n_layers=n_layers, dropout=0.2).to(device)\n",
        "rnn_optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate, weight_decay=5e-5)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(rnn_optimizer, step_size=500, gamma=0.5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "start = time.time()\n",
        "all_losses = []\n",
        "test_losses = []\n",
        "loss_avg = 0\n",
        "test_loss_avg = 0\n",
        "\n",
        "\n",
        "print(\"Training for %d epochs...\" % n_epochs)\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    scheduler.step()\n",
        "    loss = train(rnn, *load_random_batch(train_text, chunk_len, batch_size), rnn_optimizer, criterion)\n",
        "    loss_avg += loss\n",
        "    \n",
        "    test_loss = eval_test(rnn, *load_random_batch(test_text, chunk_len, batch_size))\n",
        "    test_loss_avg += test_loss\n",
        "\n",
        "    if epoch % print_every == 0:\n",
        "        print('[%s (%d %d%%) train loss: %.4f, test_loss: %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss, test_loss))\n",
        "        print(generate(rnn, 'Wh', 100, device=device), '\\n')\n",
        "\n",
        "    if epoch % plot_every == 0:\n",
        "        all_losses.append(loss_avg / plot_every)\n",
        "        test_losses.append(test_loss_avg / plot_every)\n",
        "        loss_avg = 0\n",
        "        test_loss_avg = 0"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 5000 epochs...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0m 51s (50 1%) train loss: 1.9918, test_loss: 2.0318]\n",
            "Whatzirh facty as grage shus heat:\n",
            "Good ford.\n",
            "\n",
            "CLALES:\n",
            "It enape my and so spordy.\n",
            "\n",
            "PALI:\n",
            "And thought,  \n",
            "\n",
            "[1m 41s (100 2%) train loss: 1.7669, test_loss: 1.7823]\n",
            "Whinks, which heart for your condender,\n",
            "If you!\n",
            "\n",
            "SHORMOCESS:\n",
            "Derish I hear betitherse be itselman,\n",
            "Doc \n",
            "\n",
            "[2m 31s (150 3%) train loss: 1.6225, test_loss: 1.7073]\n",
            "When, and soltem,\n",
            "the man, goine of this at this grraciant of a lit it firms\n",
            "More to the death and sle \n",
            "\n",
            "[3m 25s (200 4%) train loss: 1.5884, test_loss: 1.6498]\n",
            "Where in thee.\n",
            "\n",
            "ELIMACH:\n",
            "Sucourse. Willian's truther islement to a not\n",
            "into an inner.\n",
            "\n",
            "Both:\n",
            "What: and \n",
            "\n",
            "[4m 16s (250 5%) train loss: 1.5429, test_loss: 1.6042]\n",
            "Whis again, and it any firment of have.\n",
            "\n",
            "GROTED:\n",
            "I have enound with the old of the mean of I.\n",
            "\n",
            "PRINCE  \n",
            "\n",
            "[5m 9s (300 6%) train loss: 1.4934, test_loss: 1.5814]\n",
            "Whilths me so new ab's any works\n",
            "I say gone taught of it this is anre as fiends\n",
            "Fears your saying in t \n",
            "\n",
            "[5m 59s (350 7%) train loss: 1.5037, test_loss: 1.5714]\n",
            "What liding armle in to such by to:\n",
            "Shall me the world made black and out mething.\n",
            "\n",
            "CASSIO:\n",
            "Wear the w \n",
            "\n",
            "[6m 51s (400 8%) train loss: 1.4861, test_loss: 1.5787]\n",
            "When the court eneces are our name troal\n",
            "Shall gentle fool our putrice pays be bandchime\n",
            "Being soon th \n",
            "\n",
            "[7m 42s (450 9%) train loss: 1.4773, test_loss: 1.5553]\n",
            "What, but with that words like thee,\n",
            "Shall know our a chasted seven wiseless with streeN,\n",
            "Good recove  \n",
            "\n",
            "[8m 32s (500 10%) train loss: 1.4825, test_loss: 1.5433]\n",
            "Whearen superstity, and with you stold\n",
            "with a lacked men well.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "The very parts a acch \n",
            "\n",
            "[9m 23s (550 11%) train loss: 1.3927, test_loss: 1.4908]\n",
            "When goes, I discourse this rion and deserving\n",
            "That again there, onful in the cancles are,\n",
            "Emplease to \n",
            "\n",
            "[10m 13s (600 12%) train loss: 1.4020, test_loss: 1.4849]\n",
            "When and your young good lord of me, that I beserved\n",
            "A garleasy first with some scrue best the man and \n",
            "\n",
            "[11m 3s (650 13%) train loss: 1.4100, test_loss: 1.5090]\n",
            "Who comest them with his zate, this reverend to those,\n",
            "Though he sent by my sons of the snaved wind\n",
            "An \n",
            "\n",
            "[11m 53s (700 14%) train loss: 1.3896, test_loss: 1.4748]\n",
            "When he hath even!\n",
            "I'll be not good to hear sun, which being in scellens.\n",
            "\n",
            "CORTHUMm ASTAVERTINE:\n",
            "So sh \n",
            "\n",
            "[12m 43s (750 15%) train loss: 1.3848, test_loss: 1.4983]\n",
            "What shallow have gone that hath waters, and drown my good\n",
            "consider: he that a noble Citizens, say'd!\n",
            " \n",
            "\n",
            "[13m 33s (800 16%) train loss: 1.3950, test_loss: 1.4715]\n",
            "What it home hend on her fortune.\n",
            "\n",
            "CELIA:\n",
            "Thou hast a sea?\n",
            "\n",
            "BARDOLPH PROSSELA:\n",
            "I will hark up fine, se \n",
            "\n",
            "[14m 23s (850 17%) train loss: 1.3796, test_loss: 1.4913]\n",
            "When all the way she; on his heavens,\n",
            "The news the gods we come of.\n",
            "\n",
            "LYSANDER:\n",
            "Be now he have with no  \n",
            "\n",
            "[15m 14s (900 18%) train loss: 1.3768, test_loss: 1.4787]\n",
            "Whe loss she bring, war than you will not please?\n",
            "See have banish'd on me, my lord,\n",
            "As much all such a \n",
            "\n",
            "[16m 3s (950 19%) train loss: 1.3738, test_loss: 1.5053]\n",
            "When he\n",
            "mounts to do something, and spirit me not a, I\n",
            "do a cannot lay what reason it with loved that  \n",
            "\n",
            "[16m 54s (1000 20%) train loss: 1.3850, test_loss: 1.4719]\n",
            "Who comes him prolicious' deed\n",
            "Lord for the poison, 'tis truth and are now,\n",
            "As thou vills bring thee,  \n",
            "\n",
            "[17m 46s (1050 21%) train loss: 1.3555, test_loss: 1.4299]\n",
            "Whiles not to desert being rogue,\n",
            "Who thou art breeding worse.\n",
            "\n",
            "PETRUCHIO:\n",
            "How now! Your religious sai \n",
            "\n",
            "[18m 35s (1100 22%) train loss: 1.3528, test_loss: 1.4620]\n",
            "What he did be Brutus; she's here as I will repent.\n",
            "\n",
            "FLUELLEN:\n",
            "That's her satisfactions have their han \n",
            "\n",
            "[19m 25s (1150 23%) train loss: 1.3404, test_loss: 1.4360]\n",
            "What flesh shall not have no 'ergo?\n",
            "\n",
            "OTHELLO:\n",
            "I wonder-wolk after. Nemular see,\n",
            "By love, and all the h \n",
            "\n",
            "[20m 16s (1200 24%) train loss: 1.3386, test_loss: 1.4597]\n",
            "Where on our words again\n",
            "To pleasure my ord to be pastly.\n",
            "\n",
            "PAGE:\n",
            "Know, I say, come, I think as much is \n",
            "\n",
            "[21m 5s (1250 25%) train loss: 1.3287, test_loss: 1.4373]\n",
            "What men that you.\n",
            "\n",
            "CORIOLANUS:\n",
            "I would my prayers were what thou sawest?\n",
            "\n",
            "MISTRESS PAGE:\n",
            "Ay, that the \n",
            "\n",
            "[21m 56s (1300 26%) train loss: 1.3136, test_loss: 1.4271]\n",
            "Which is I may as man's noble lord;\n",
            "First in fine hour make her fellow and say 'ay.\n",
            "\n",
            "PAROLLES:\n",
            "Not a m \n",
            "\n",
            "[22m 45s (1350 27%) train loss: 1.3146, test_loss: 1.4431]\n",
            "Why.\n",
            "\n",
            "TIMON:\n",
            "Look, I am no tongue to show;\n",
            "For I have weep in such as burdey; by the\n",
            "rest all the word \n",
            "\n",
            "[23m 34s (1400 28%) train loss: 1.3228, test_loss: 1.4366]\n",
            "Where I be constawn'd\n",
            "Humberlance than ever but to inferable\n",
            "That did this traitor ear my servised\n",
            "Ben \n",
            "\n",
            "[24m 23s (1450 28%) train loss: 1.3288, test_loss: 1.4524]\n",
            "Who takesty sovereign; and the first now,\n",
            "Be cured with an hour eyes and princes.\n",
            "\n",
            "BAPTOT:\n",
            "Good sir, t \n",
            "\n",
            "[25m 12s (1500 30%) train loss: 1.3324, test_loss: 1.4418]\n",
            "What 8ercrew heaws of heaven in sake\n",
            "What tempt is the world for thy gentleman,\n",
            "Aching even now of thi \n",
            "\n",
            "[26m 1s (1550 31%) train loss: 1.2949, test_loss: 1.4249]\n",
            "Which shall prove the reason of friends of mine;\n",
            "In all the lime be fellow'd for the door of Herry\n",
            "One \n",
            "\n",
            "[26m 51s (1600 32%) train loss: 1.3085, test_loss: 1.4108]\n",
            "Whom I\n",
            "know it in the watchy, as a poor people-git's\n",
            "ascent with me, my lords will say so: I would\n",
            "be  \n",
            "\n",
            "[27m 41s (1650 33%) train loss: 1.2680, test_loss: 1.4139]\n",
            "Who bids at the world take him and join on the\n",
            "capable to occasion but threatenates, in bears\n",
            "Who will \n",
            "\n",
            "[28m 30s (1700 34%) train loss: 1.3023, test_loss: 1.4276]\n",
            "What would resolve none, double use my right\n",
            "Beholding; where is the flowring lirgunes\n",
            "That to me the  \n",
            "\n",
            "[29m 21s (1750 35%) train loss: 1.3157, test_loss: 1.3919]\n",
            "Who did you carriin her noble couns in my fond are\n",
            "unsicence to the lady, and so ground to thright.\n",
            "\n",
            "S \n",
            "\n",
            "[30m 9s (1800 36%) train loss: 1.3104, test_loss: 1.4251]\n",
            "What standers?\n",
            "\n",
            "WARWICK:\n",
            "Both is a thing dost old word for your reverence.\n",
            "This is no sail, that say y \n",
            "\n",
            "[30m 59s (1850 37%) train loss: 1.2864, test_loss: 1.4151]\n",
            "What's not fought in death.\n",
            "\n",
            "REGAN:\n",
            "Go that me interstant; but if we forward death.\n",
            "\n",
            "ANTONY:\n",
            "Why, ther \n",
            "\n",
            "[31m 49s (1900 38%) train loss: 1.2979, test_loss: 1.4088]\n",
            "Whearer, make your commendations: here's my discord.\n",
            "\n",
            "BIRON:\n",
            "I did know not, fare him afterged me.\n",
            "Goo \n",
            "\n",
            "[32m 37s (1950 39%) train loss: 1.2928, test_loss: 1.4019]\n",
            "Where is death my youth into that sense,\n",
            "That we in the devil his lowers, sickly married;\n",
            "For he had d \n",
            "\n",
            "[33m 25s (2000 40%) train loss: 1.2676, test_loss: 1.3743]\n",
            "Whiles cut it for your father,\n",
            "I have talk'd unto some poor wit ill: and\n",
            "there loyalty from your whom  \n",
            "\n",
            "[34m 13s (2050 41%) train loss: 1.2870, test_loss: 1.4368]\n",
            "Who, the rest, with an object your hate of king\n",
            "Was done all of twenty.\n",
            "\n",
            "KING RICHARD III:\n",
            "A month pas \n",
            "\n",
            "[35m 2s (2100 42%) train loss: 1.3046, test_loss: 1.3960]\n",
            "Whose traitor quench this nature, cannot do\n",
            "For measure; both on't, I pray you, and it strike\n",
            "me to af \n",
            "\n",
            "[35m 52s (2150 43%) train loss: 1.2772, test_loss: 1.4235]\n",
            "What I hear, my lord, a lady's lover, that it shows.\n",
            "Were you gracious see the moon's that!\n",
            "\n",
            "CLEOPATRA \n",
            "\n",
            "[36m 43s (2200 44%) train loss: 1.2903, test_loss: 1.4066]\n",
            "Where I did bring more men\n",
            "As are not more reason to punish me; and\n",
            "Is stated I heard thy duke and man \n",
            "\n",
            "[37m 37s (2250 45%) train loss: 1.2705, test_loss: 1.4111]\n",
            "Whore infinite sweet.\n",
            "\n",
            "EMILIA:\n",
            "Parence me with this prince to-night. The best most weary which\n",
            "have ri \n",
            "\n",
            "[38m 27s (2300 46%) train loss: 1.2866, test_loss: 1.3845]\n",
            "When we will be every soldiers down old home\n",
            "Is as compassing and live as you.\n",
            "\n",
            "BANQUO:\n",
            "Peace, fie!\n",
            "A  \n",
            "\n",
            "[39m 19s (2350 47%) train loss: 1.2777, test_loss: 1.3822]\n",
            "Why begins the means to passage her therefore.\n",
            "\n",
            "SEBASTIAN:\n",
            "Tell me for some thousand pardons for your  \n",
            "\n",
            "[40m 10s (2400 48%) train loss: 1.2519, test_loss: 1.4095]\n",
            "What blessed purpose receives that stays well\n",
            "calls on him, that he can learn no heartest, in the good \n",
            "\n",
            "[41m 2s (2450 49%) train loss: 1.2746, test_loss: 1.4030]\n",
            "Where an hour fald for the world.\n",
            "\n",
            "PERICLES:\n",
            "He'll brave thee for a woman of such, and the\n",
            "world stran \n",
            "\n",
            "[41m 52s (2500 50%) train loss: 1.3052, test_loss: 1.4108]\n",
            "What dogs the night ignorant\n",
            "Mortalion.\n",
            "\n",
            "THURIO:\n",
            "Then in hand of may cuts him for a leavely\n",
            "conference \n",
            "\n",
            "[42m 42s (2550 51%) train loss: 1.2741, test_loss: 1.4110]\n",
            "Which what consent is too hand eased in an oath!\n",
            "I will be drawn and others' takes reputation with\n",
            "him \n",
            "\n",
            "[43m 34s (2600 52%) train loss: 1.2545, test_loss: 1.3858]\n",
            "Which more dear as in this holy\n",
            "Dares do receive thee too; each wars\n",
            "That for his pleasure, let a love \n",
            "\n",
            "[44m 25s (2650 53%) train loss: 1.2595, test_loss: 1.3943]\n",
            "Wherefore, let him after it.\n",
            "\n",
            "KING HENRY VI:\n",
            "'Tis thernly to my father; I must never\n",
            "be not a prentanc \n",
            "\n",
            "[45m 15s (2700 54%) train loss: 1.2476, test_loss: 1.3976]\n",
            "What she did come to gentle trace.\n",
            "\n",
            "Servant:\n",
            "Alas, my lord,\n",
            "Lay earth, from your mile in fire.\n",
            "\n",
            "DUKE V \n",
            "\n",
            "[46m 6s (2750 55%) train loss: 1.2796, test_loss: 1.3885]\n",
            "Whoom I have fortuned your friends:\n",
            "I shall neither make them ever that\n",
            "Happier with the complaints of \n",
            "\n",
            "[46m 57s (2800 56%) train loss: 1.2840, test_loss: 1.4315]\n",
            "Whirk!\n",
            "\n",
            "PISTOL:\n",
            "The garden shelter, I think now not to show him again.\n",
            "\n",
            "DEMETRIUS:\n",
            "Foll like thy maste \n",
            "\n",
            "[47m 48s (2850 56%) train loss: 1.2567, test_loss: 1.3887]\n",
            "What offence shall be discredited?\n",
            "So I can so, I prithee, foolish tears.\n",
            "\n",
            "DOMITIUS ENOBARBUS:\n",
            "I shall \n",
            "\n",
            "[48m 40s (2900 57%) train loss: 1.2500, test_loss: 1.3944]\n",
            "When I was customed with your pockets.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "With Friar Alentidius, we have the beauty\n",
            "inf \n",
            "\n",
            "[49m 31s (2950 59%) train loss: 1.2768, test_loss: 1.3882]\n",
            "Where, ha!\n",
            "\n",
            "HORTENSIO:\n",
            "Sir, art thou done? I protess down and meet,\n",
            "By make them for I doubt the flowe \n",
            "\n",
            "[50m 21s (3000 60%) train loss: 1.2798, test_loss: 1.3944]\n",
            "Where a washes of interpret,\n",
            "That you behold the neighy to your royal,\n",
            "And such awake! And so the best \n",
            "\n",
            "[51m 14s (3050 61%) train loss: 1.2634, test_loss: 1.4046]\n",
            "Where obedience here the passion does\n",
            "her lips, make condition, under but a most,\n",
            "I'll blame her mothe \n",
            "\n",
            "[52m 4s (3100 62%) train loss: 1.2490, test_loss: 1.3929]\n",
            "Whias gible men or else\n",
            "Of the world will die in your hand, the other by\n",
            "a man.\n",
            "\n",
            "BARDOLPH:\n",
            "I cannot be \n",
            "\n",
            "[52m 55s (3150 63%) train loss: 1.2760, test_loss: 1.4134]\n",
            "Which pompetitudate boasted that appared when the\n",
            "itself and the childish parting and hours and no mor \n",
            "\n",
            "[53m 45s (3200 64%) train loss: 1.2688, test_loss: 1.3787]\n",
            "Whores how with Hector be the nearest.\n",
            "\n",
            "TITUS ANDRONICUS:\n",
            "A man and more sing to assail himself:\n",
            "To co \n",
            "\n",
            "[54m 35s (3250 65%) train loss: 1.2502, test_loss: 1.3770]\n",
            "Whector, and let of place by your contnues\n",
            "To the together gross of Richmond master:\n",
            "One frontens of e \n",
            "\n",
            "[55m 29s (3300 66%) train loss: 1.2717, test_loss: 1.3909]\n",
            "Which needs do bleeds to thee in one hour\n",
            "In power your truth, imwarring to the excellent;\n",
            "Where he is \n",
            "\n",
            "[56m 17s (3350 67%) train loss: 1.2532, test_loss: 1.3981]\n",
            "White for thee, hath not a wound redeem'd\n",
            "The worms upon the church of this and over-hand\n",
            "Did Jessal t \n",
            "\n",
            "[57m 8s (3400 68%) train loss: 1.2602, test_loss: 1.3942]\n",
            "Whore eras be married\n",
            "With brother's neck and truly; for this here\n",
            "In his best seat a precious lover,\n",
            " \n",
            "\n",
            "[57m 58s (3450 69%) train loss: 1.2529, test_loss: 1.3808]\n",
            "Which hath to search me to a measure,\n",
            "Send them to die we are here to me,\n",
            "When them appear be gone in  \n",
            "\n",
            "[58m 48s (3500 70%) train loss: 1.2872, test_loss: 1.3967]\n",
            "Whore--\n",
            "\n",
            "CAMILLO:\n",
            "None spark, madam, I see my heart, lose me, anon,\n",
            "I have so idleness.\n",
            "\n",
            "DOMITIUS ENOB \n",
            "\n",
            "[59m 38s (3550 71%) train loss: 1.2738, test_loss: 1.4000]\n",
            "What knack how I was the great of love.\n",
            "\n",
            "CAPBER'C:\n",
            "I must be past\n",
            "Soldier in the world. For this oblia \n",
            "\n",
            "[60m 29s (3600 72%) train loss: 1.2481, test_loss: 1.3878]\n",
            "Whore word to a man\n",
            "Comes what you have been known to our treams;\n",
            "I give you a word and all for heavy  \n",
            "\n",
            "[61m 21s (3650 73%) train loss: 1.2594, test_loss: 1.3922]\n",
            "Where's an Adoraxion ever, I'll be king,\n",
            "That she hath follow'd such a nooding bult\n",
            "And make his name  \n",
            "\n",
            "[62m 13s (3700 74%) train loss: 1.2474, test_loss: 1.3865]\n",
            "Who shall the tagerier of my trop--through their body\n",
            "Should make his comfort entertain their botch.\n",
            "\n",
            " \n",
            "\n",
            "[63m 2s (3750 75%) train loss: 1.2487, test_loss: 1.4139]\n",
            "What I shall speak with me and further.\n",
            "\n",
            "POSTHUMUS LEONATUS:\n",
            "What's that, I serve thy son Tamora?\n",
            "Is t \n",
            "\n",
            "[63m 54s (3800 76%) train loss: 1.2466, test_loss: 1.4084]\n",
            "Which instracted made to stream; who ours, bug talse\n",
            "Signior to they content to the action of my daugh \n",
            "\n",
            "[64m 45s (3850 77%) train loss: 1.2441, test_loss: 1.4060]\n",
            "Why I would speak to me, I am sail;\n",
            "but I am for that ciatewer: so she would be an\n",
            "obscure of the moon \n",
            "\n",
            "[65m 35s (3900 78%) train loss: 1.2540, test_loss: 1.4037]\n",
            "Which undertake the doory seem\n",
            "Of every preparation in the which\n",
            "In the countenance to provide to natu \n",
            "\n",
            "[66m 25s (3950 79%) train loss: 1.2516, test_loss: 1.3725]\n",
            "When I do not\n",
            "To draw her cried the old man slain.\n",
            "\n",
            "FALSTAFF:\n",
            "Why, my lord, yet I do believe you; and  \n",
            "\n",
            "[67m 18s (4000 80%) train loss: 1.2454, test_loss: 1.4227]\n",
            "Whoris Richard he is of valued Troilus:\n",
            "There's not a little and olper days of men.\n",
            "\n",
            "MARK ANTONY:\n",
            "No m \n",
            "\n",
            "[68m 10s (4050 81%) train loss: 1.2588, test_loss: 1.3922]\n",
            "Whom would not bear but here;\n",
            "Who farewell longer and so still in thine:\n",
            "And now to be strong, for for \n",
            "\n",
            "[69m 1s (4100 82%) train loss: 1.2652, test_loss: 1.3977]\n",
            "What looks and griefs, that spoking: whulted dobbition want\n",
            "To this my overtent; he's an enemies,\n",
            "As a \n",
            "\n",
            "[69m 52s (4150 83%) train loss: 1.2399, test_loss: 1.3838]\n",
            "Whom I will not meet a gentle tongue to marry,\n",
            "very your worship.\n",
            "\n",
            "HELE:\n",
            "A washionacle, do! 'tis not g \n",
            "\n",
            "[70m 41s (4200 84%) train loss: 1.2478, test_loss: 1.4024]\n",
            "What he was like a gamoffer; she makes a sign\n",
            "Was a maid rascalf as abroad, by any complaint.\n",
            "\n",
            "PANDARU \n",
            "\n",
            "[71m 32s (4250 85%) train loss: 1.2401, test_loss: 1.3816]\n",
            "Why, that I please me, and your reformed sons\n",
            "Will buy the crown.\n",
            "\n",
            "ROSALIND:\n",
            "Yes, Herendo and your wit \n",
            "\n",
            "[72m 21s (4300 86%) train loss: 1.2606, test_loss: 1.3659]\n",
            "What must displease?\n",
            "What tells you leaves the very estedamply\n",
            "On 'twere and bold, if he then have dea \n",
            "\n",
            "[73m 10s (4350 87%) train loss: 1.2414, test_loss: 1.3919]\n",
            "Whose quality hath death to leaw them.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Why, on my part, I would be mercenting the bes \n",
            "\n",
            "[74m 2s (4400 88%) train loss: 1.2458, test_loss: 1.3968]\n",
            "What is the course;\n",
            "In a most grace to the daughters' sake.\n",
            "\n",
            "SHALLOW:\n",
            "He that didst please your liver  \n",
            "\n",
            "[74m 53s (4450 89%) train loss: 1.2358, test_loss: 1.4101]\n",
            "Where forbason the death of England.\n",
            "\n",
            "MARK ANTONY:\n",
            "Sug high my hand; and so much in his side and heart \n",
            "\n",
            "[75m 44s (4500 90%) train loss: 1.2478, test_loss: 1.3815]\n",
            "What I have dared to death;\n",
            "And therefore in a more be past so else.\n",
            "\n",
            "GLOUCESTER:\n",
            "Plantagenet! I will  \n",
            "\n",
            "[76m 34s (4550 91%) train loss: 1.2345, test_loss: 1.4087]\n",
            "Whores as turred time to weigh afoot.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "The city fearful titches, that of this which\n",
            "m \n",
            "\n",
            "[77m 22s (4600 92%) train loss: 1.2591, test_loss: 1.3938]\n",
            "Which will I make his marriage\n",
            "Can choler than the ere articles.\n",
            "\n",
            "NORTHUMBERLAND:\n",
            "The pure cols is to  \n",
            "\n",
            "[78m 13s (4650 93%) train loss: 1.2320, test_loss: 1.3900]\n",
            "Whore you since they lived, let him fetch with him.\n",
            "\n",
            "NLONINO:\n",
            "Like him so much would fly buy formance. \n",
            "\n",
            "[79m 2s (4700 94%) train loss: 1.2542, test_loss: 1.4038]\n",
            "What satisfaction was the feast?\n",
            "O I should be content, and bestrown me,\n",
            "Wronged by the duke of some w \n",
            "\n",
            "[79m 52s (4750 95%) train loss: 1.2520, test_loss: 1.3987]\n",
            "What shrift Sir Emploring-fill'd at him,\n",
            "The world and the people was to me,\n",
            "As pledged standing in hi \n",
            "\n",
            "[80m 42s (4800 96%) train loss: 1.2472, test_loss: 1.3757]\n",
            "What your worship shall\n",
            "learn by me.\n",
            "\n",
            "CLAUDIO:\n",
            "I am sour as names two bhefits and many villains;\n",
            "I hav \n",
            "\n",
            "[81m 34s (4850 97%) train loss: 1.2618, test_loss: 1.3639]\n",
            "What none but bed are pain to be\n",
            "above no pomp, thou camest what he were\n",
            "And by the flourishest of man \n",
            "\n",
            "[82m 22s (4900 98%) train loss: 1.2662, test_loss: 1.4067]\n",
            "Which of the air:\n",
            "Are you a cook appear before to me?\n",
            "\n",
            "KING:\n",
            "Now, Lord, is fit his face, man half a wa \n",
            "\n",
            "[83m 14s (4950 99%) train loss: 1.2698, test_loss: 1.3684]\n",
            "Wh does his good\n",
            "To put the enaminate garden form'd!\n",
            "East thou shalt find him out of sacred fancy,\n",
            "And \n",
            "\n",
            "[84m 1s (5000 100%) train loss: 1.2322, test_loss: 1.3839]\n",
            "Which race for knight-owells;\n",
            "If faith in heart of present favour,\n",
            "Thou makest me at a more of the big \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyQU1wD62SxD"
      },
      "source": [
        "# save network\n",
        "torch.save(rnn.state_dict(), './rnn_generator.pth')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kI7BMfw2SxG"
      },
      "source": [
        "# Plot the Training and Test Losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFrdEDpE2SxH",
        "outputId": "5cc624c9-e1f4-4cf8-ea85-fb52c8ede393",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(all_losses)\n",
        "plt.plot(test_losses, color='r')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fb264d2f748>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z8YS3Ge2SxJ"
      },
      "source": [
        "# Evaluate text generation\n",
        "\n",
        "Check what the outputted text looks like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ-WcvED2SxJ",
        "outputId": "7c3028e6-8662-4302-b41b-2e7935fd0eea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(evaluate(rnn, prime_str='Th', predict_len=1000))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thebore,\n",
            "Till princes the breath of all the younger is\n",
            "Than him and honesty servants.\n",
            "\n",
            "PAULINA:\n",
            "But shall I draw my jealousy more to foul?\n",
            "Three Richard, be advanced? But, be you once?\n",
            "\n",
            "KATHARINE:\n",
            "You are too faith. Often to you, hear you like the\n",
            "estephed my vile strange one that you desire your fair\n",
            "Richard; we cannot plead for Egyant,\n",
            "Wherefore myself are in our dangerous steller;\n",
            "My mother I'll be mustice from the state.\n",
            "no word I know your stroke and never tongue so\n",
            "beguiled within the grace of pot, repair to this.\n",
            "\n",
            "CARDINAL WOLSEY:\n",
            "One judgment was or strait's love\n",
            "And spend to any conction: having more more\n",
            "than coming in the for the time--\n",
            "\n",
            "CLAUDIO:\n",
            "Madam, your hair and would say that I do!\n",
            "\n",
            "AESEES:\n",
            "And I would he would take my heart for you.\n",
            "\n",
            "DROMIO OF SYRACUSE:\n",
            "Say any of your please interate?\n",
            "\n",
            "VINCENTIO:\n",
            "I will speak this oblicible change of my fortunes\n",
            "to choose into my master. This honourable state\n",
            "Is spoil'd there shall go send him, and at Antony.\n",
            "\n",
            "SLENDER:\n",
            "I will not sing \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oq6MCO4W2SxM"
      },
      "source": [
        "# Hyperparameter Tuning\n",
        "\n",
        "Some things you should try to improve your network performance are:\n",
        "- Different RNN types. Switch the basic RNN network in your model to a GRU and LSTM to compare all three.\n",
        "- Try adding 1 or two more layers\n",
        "- Increase the hidden layer size\n",
        "- Changing the learning rate\n",
        "\n",
        "**TODO:** Try changing the RNN type and hyperparameters. Record your results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3-UHg7e2SxM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}